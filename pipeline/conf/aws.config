/*
========================================================================================
    AWS Configuration for GeneXOmics Perturb-Seq Pipeline
========================================================================================
    This configuration enables the pipeline to run on AWS infrastructure using:
    - AWS Batch for compute orchestration
    - Amazon S3 for data storage and work directory
    - Amazon ECR for container registry
    
    Usage: nextflow run main.nf -profile aws
    
    Prerequisites:
    1. AWS credentials configured (AWS CLI or IAM role)
    2. AWS Batch compute environment and job queue created
    3. S3 bucket for work directory created
    4. Docker images pushed to ECR
========================================================================================
*/

// ==================== AWS Configuration ====================

params {
    // AWS-specific parameters (can be overridden via command line or config)
    aws_region           = System.getenv('AWS_REGION') ?: 'us-east-1'
    aws_queue            = System.getenv('AWS_BATCH_QUEUE') ?: 'genexomics-pipeline-queue'
    aws_cli_path         = '/usr/local/aws-cli/v2/current/bin/aws'
    
    // S3 bucket for work directory (must exist and have proper permissions)
    work_bucket          = System.getenv('WORK_BUCKET') ?: 's3://genexomics-nextflow-work'
    
    // ECR registry (format: <account-id>.dkr.ecr.<region>.amazonaws.com)
    ecr_registry         = System.getenv('ECR_REGISTRY') ?: null  // Must be set!
    
    // Use spot instances for cost optimization (recommended for non-critical tasks)
    use_spot_instances   = true
}

// AWS region
aws {
    region = params.aws_region
    batch {
        cliPath = params.aws_cli_path
        maxParallelTransfers = 8
        maxTransferAttempts = 5
        delayBetweenAttempts = '10 sec'
    }
    client {
        maxConnections = 20
        connectionTimeout = 10000
        uploadMaxThreads = 4
        uploadChunkSize = '100MB'
        uploadStorageClass = 'INTELLIGENT_TIERING'  // Cost optimization
        storageEncryption = 'AES256'  // Security compliance
    }
}

// ==================== Executor Configuration ====================

process {
    executor = 'awsbatch'
    queue = params.aws_queue
    scratch = false  // Use container scratch space, not EBS volumes
    
    // Default error strategy for cloud failures
    errorStrategy = { 
        task.exitStatus in [143,137,104,134,139,255] ? 'retry' :
        task.attempt < 3 ? 'retry' : 'finish' 
    }
    maxRetries = 3
}

// Work directory in S3
workDir = params.work_bucket

// AWS Batch executor settings
executor {
    name = 'awsbatch'
    queueSize = 20  // Max concurrent jobs
    pollInterval = '30 sec'
    submitRateLimit = '10 sec'
    exitReadTimeout = '5 min'
}

// ==================== Container Registry ====================

docker {
    enabled = true
    registry = params.ecr_registry
}

// ==================== Process-Specific AWS Batch Configuration ====================

process {
    
    // BCL to FASTQ conversion
    withName: 'BCL2FASTQ' {
        container = params.ecr_registry ? "${params.ecr_registry}/genexomics/bcl2fastq:1.0" : 'genexomics/bcl2fastq:1.0'
        cpus = 8
        memory = 16.GB
        time = 6.h
        queue = params.aws_queue
        // Use on-demand for critical primary analysis
        queue = params.use_spot_instances ? "${params.aws_queue}-spot" : params.aws_queue
    }
    
    // Quality control
    withName: 'FASTQC' {
        container = params.ecr_registry ? "${params.ecr_registry}/genexomics/fastqc:1.0" : 'genexomics/fastqc:1.0'
        cpus = 2
        memory = 4.GB
        time = 2.h
        // FastQC is quick and cheap - can use spot instances
        queue = params.use_spot_instances ? "${params.aws_queue}-spot" : params.aws_queue
    }
    
    // Cell Ranger Count (single-modal)
    withName: 'CELLRANGER_COUNT' {
        container = params.ecr_registry ? "${params.ecr_registry}/genexomics/cellranger:1.0" : 'genexomics/cellranger:1.0'
        cpus = 16
        memory = 64.GB
        time = 12.h
        // Use on-demand for reliability - Cell Ranger is critical and expensive to retry
        queue = params.aws_queue
        errorStrategy = 'retry'
        maxRetries = 2
    }
    
    // Cell Ranger Multi (multi-modal)
    withName: 'CELLRANGER_MULTI' {
        container = params.ecr_registry ? "${params.ecr_registry}/genexomics/cellranger:1.0" : 'genexomics/cellranger:1.0'
        cpus = 16
        memory = 64.GB
        time = 12.h
        // Use on-demand for reliability
        queue = params.aws_queue
        errorStrategy = 'retry'
        maxRetries = 2
    }
    
    // CRISPR guide assignment
    withName: 'GUIDE_ASSIGNMENT' {
        container = params.ecr_registry ? "${params.ecr_registry}/genexomics/analysis:1.0" : 'genexomics/analysis:1.0'
        cpus = 4
        memory = 16.GB
        time = 4.h
        queue = params.use_spot_instances ? "${params.aws_queue}-spot" : params.aws_queue
    }
    
    // QC filtering
    withName: 'QC_FILTER' {
        container = params.ecr_registry ? "${params.ecr_registry}/genexomics/scanpy-qc:1.0" : 'genexomics/scanpy-qc:1.0'
        cpus = 4
        memory = 16.GB
        time = 4.h
        queue = params.use_spot_instances ? "${params.aws_queue}-spot" : params.aws_queue
    }
    
    // Clustering analysis
    withName: 'CLUSTERING' {
        container = params.ecr_registry ? "${params.ecr_registry}/genexomics/analysis:1.0" : 'genexomics/analysis:1.0'
        cpus = 8
        memory = 32.GB
        time = 6.h
        queue = params.use_spot_instances ? "${params.aws_queue}-spot" : params.aws_queue
    }
}

// ==================== Execution Monitoring ====================

trace {
    enabled = true
    file = "${params.outdir}/pipeline_info/trace.txt"
    fields = 'task_id,hash,native_id,name,status,exit,submit,start,complete,duration,realtime,cpus,memory,rss,vmem,peak_rss,peak_vmem,time,queue,attempt,container'
}

timeline {
    enabled = true
    file = "${params.outdir}/pipeline_info/timeline.html"
}

report {
    enabled = true
    file = "${params.outdir}/pipeline_info/report.html"
}

// ==================== Notes ====================
/*
Setup Instructions:

1. Create AWS Batch compute environment and queues:
   - On-demand queue: genexomics-pipeline-queue
   - Spot queue: genexomics-pipeline-queue-spot (optional, for cost savings)

2. Create S3 bucket for work directory:
   aws s3 mb s3://genexomics-nextflow-work --region us-east-1

3. Push Docker images to ECR:
   aws ecr get-login-password | docker login --username AWS --password-stdin <account>.dkr.ecr.us-east-1.amazonaws.com
   docker tag genexomics/cellranger:1.0 <account>.dkr.ecr.us-east-1.amazonaws.com/genexomics/cellranger:1.0
   docker push <account>.dkr.ecr.us-east-1.amazonaws.com/genexomics/cellranger:1.0
   # Repeat for all containers

4. Run pipeline:
   export AWS_REGION=us-east-1
   export AWS_BATCH_QUEUE=genexomics-pipeline-queue
   export WORK_BUCKET=s3://genexomics-nextflow-work
   export ECR_REGISTRY=<account>.dkr.ecr.us-east-1.amazonaws.com
   
   nextflow run main.nf \
     -profile aws \
     --fastq_dir s3://genexomics-data/runs/run_20260118 \
     --reference s3://genexomics-references/refdata-gex-GRCh38-2024-A \
     --outdir s3://genexomics-results/run_20260118

Cost Optimization:
- Spot instances save 70-90% on compute costs
- INTELLIGENT_TIERING storage saves on S3 costs
- Proper resource allocation prevents over-provisioning
- Retry logic prevents wasted computation

For GeneXOmics (~8 runs/week, ~180GB/week):
- Estimated cost: $50-150 per run (with spot instances)
- Total monthly: $1,600-4,800 (vs $5,000+ on-demand only)
*/
